{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 01:56:37.515872: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-28 01:56:38.450455: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-28 01:56:38.450564: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-28 01:56:38.450575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from src.DST.dst import SLOTS_DESCRIPTIONS\n",
    "from src.DST.config import CONFIG\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, AutoModelForCausalLM, T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "\n",
    "class PromptConstructor():\n",
    "    def __init__(self, \n",
    "                 config):\n",
    "        self.config = config\n",
    "        self.instructions = config[\"INSTRUCTIONS\"]\n",
    "        self.prompt_templates = config[\"PROMPT_TEMPLATES\"]\n",
    "        \n",
    "    def _get_slots_from_domains(self, domains, with_slot_description, with_req_inf_differentiation, with_all_slots):\n",
    "        # slot_description = self.config[\"slot_descrpition\"]\n",
    "        if with_all_slots:\n",
    "            domains = \"all\"\n",
    "        \n",
    "        if with_slot_description:\n",
    "            with_req_inf_differentiation = False #Slot description is the discriminator\n",
    "\n",
    "        if domains == \"all\":\n",
    "            if with_req_inf_differentiation:\n",
    "                req_slots = \", \".join(self.config[\"multiwoz21\"][\"all_requestable_slots\"])\n",
    "                inf_slots = \", \".join(self.config[\"multiwoz21\"][\"all_informable_slots\"])\n",
    "            else:\n",
    "                slots = set(self.config[\"multiwoz21\"][\"all_requestable_slots\"] + \n",
    "                            self.config[\"multiwoz21\"][\"all_informable_slots\"])\n",
    "                slots = \", \".join(slots)\n",
    "        elif not isinstance(domains, list):\n",
    "            raise ValueError(\"\"\"Provided domain should be either 'all' or list of valid domain names:\n",
    "                                - for multiwoz2.1 and 2.4: taxi, restaurant, hotel, train, attraction \n",
    "                                - for SGD: To-do\"\"\")\n",
    "        else:\n",
    "            req_slots = \"\"\n",
    "            inf_slots = \"\"\n",
    "            domain_req_slots = []\n",
    "            domain_inf_slots = []\n",
    "            for domain in domains:\n",
    "                domain_req_slots += self.config[\"multiwoz21\"][\"requestable_slots\"][domain]\n",
    "                domain_inf_slots += self.config[\"multiwoz21\"][\"informable_slots\"][domain]\n",
    "            if with_req_inf_differentiation:\n",
    "                domain_req_slots = set(domain_req_slots)\n",
    "                domain_inf_slots = set(domain_inf_slots)\n",
    "                req_slots += \", \".join(domain_req_slots)\n",
    "                inf_slots += \", \".join(domain_inf_slots)\n",
    "            else:\n",
    "                slots = set(domain_req_slots + domain_inf_slots)\n",
    "                slots = \", \".join(slots)\n",
    "\n",
    "        if with_req_inf_differentiation:\n",
    "            slots_info = f\"Requestable slots: {req_slots}\\nInformable slots: {inf_slots}\"\n",
    "        else:\n",
    "            slots_info = f\"{slots}\"\n",
    "\n",
    "        if with_slot_description:\n",
    "            slots = slots.split(\", \")\n",
    "            slots_info = \"\"\n",
    "            for slot in slots:\n",
    "                if slot not in self.config[\"multiwoz21\"][\"all_informable_slots\"]:\n",
    "                    continue\n",
    "                slots_info += f\"name: {slot}, description: {SLOTS_DESCRIPTIONS[slot]}\\n\"\n",
    "            slots_info = slots_info[:-2]\n",
    "        \n",
    "        return slots_info\n",
    "    \n",
    "    \n",
    "    def _build_prompt(self, mode=\"\", dialogue_context=\"\", ontology=\"\", slots=\"\", dialogue_acts=\"\", belief_states=\"\"):\n",
    "        prompt = \"\"\n",
    "        if mode == \"dst\":\n",
    "            instruction = self.instructions[\"instruction_with_slots\"]\n",
    "            template_variables = self.prompt_templates[\"template_with_slots\"]\n",
    "            template = PromptTemplate(input_variables= template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])\n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                     slots=slots,\n",
    "                                     dialogue_context=dialogue_context)\n",
    "            \n",
    "        elif mode == \"dst_recorrect\":\n",
    "            instruction = self.instructions[\"instruction_with_slots_recorrect\"]\n",
    "            template = self.prompt_templates[\"template_with_slots_recorrect\"]\n",
    "            template = PromptTemplate(input_variables= template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])            \n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                    slots=slots,\n",
    "                                    dialogue_context=dialogue_context,\n",
    "                                    belief_states=belief_states)\n",
    "            \n",
    "        elif mode == \"database_query\":\n",
    "            instruction = self.instructions[\"instruction_query_database\"]\n",
    "            template = self.prompt_templates[\"template_query_database\"]\n",
    "            template = PromptTemplate(input_variables= template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])\n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                    belief_states=belief_states)\n",
    "            \n",
    "        elif mode == \"response_generation\":\n",
    "            instruction = self.instructions[\"instruction_response_generation\"]\n",
    "            template = self.prompt_templates[\"template_response_generation\"]\n",
    "            template = PromptTemplate(input_variables = template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])\n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                    dialogue_acts=dialogue_acts,\n",
    "                                    dialogue_context=dialogue_context)\n",
    "        elif mode == \"dst_extracted_ontology\":\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"'mode' should be one of: [dst, dst_recorrect, database_query, response_generation]\")\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "\n",
    "class MWOZ_Dataset(PromptConstructor):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 mwoz_path,\n",
    "                 dialog_history_limit,\n",
    "                 with_slot_description,\n",
    "                 with_req_inf_differentiation,\n",
    "                 single_domain_only,\n",
    "                 with_all_slots):\n",
    "        PromptConstructor.__init__(self, config)\n",
    "        self.dataset = {\"id\":[],\n",
    "                        \"dialogue_id\":[],\n",
    "                        \"dialogue_context\":[],\n",
    "                        \"turn\":[],\n",
    "                        \"prompt\":[],\n",
    "                        \"domains\":[],\n",
    "                        \"gold_bs\":[],\n",
    "                        \"gold_act\":[],\n",
    "                        \"gold_response\":[],\n",
    "                        \"gold_database_result\":[],\n",
    "                        }\n",
    "        self.all_data, self.testfiles = self._get_mwoz_data(mwoz_path)\n",
    "        self.idx = 0\n",
    "        self.dialog_history_limit = dialog_history_limit\n",
    "        self.single_domain_only = single_domain_only\n",
    "        self.with_slot_description = with_slot_description\n",
    "        self.with_req_inf_differentiation = with_req_inf_differentiation\n",
    "        self.with_all_slots = with_all_slots\n",
    "\n",
    "        print(\"Processing mwoz...\")\n",
    "        for sample in tqdm(self.all_data):\n",
    "            if sample in self.testfiles:\n",
    "                dialogue_log = self.all_data[sample][\"log\"]\n",
    "                self._process_dialogue_log(sample=sample,\n",
    "                                           dialogue_log=dialogue_log)\n",
    "\n",
    "        self.dataset = pd.DataFrame(self.dataset)\n",
    "        if single_domain_only:\n",
    "            for index, row in tqdm(self.dataset.iterrows()):\n",
    "                if len(row[\"domains\"]) != 1:\n",
    "                    self.dataset.drop(index, inplace=True)\n",
    "\n",
    "    def _get_mwoz_data(self, mwoz_path):\n",
    "        data_path = os.path.join(mwoz_path, \"data.json\")\n",
    "        testListFile_path = os.path.join(mwoz_path, \"testListFile.txt\")\n",
    "\n",
    "        with open(data_path, \"r\") as f:\n",
    "            all_data = json.load(f)\n",
    "            \n",
    "        with open(testListFile_path, \"r\") as f:\n",
    "            testfiles = f.read()\n",
    "        testfiles = testfiles.split(\"\\n\")\n",
    "        return all_data, testfiles\n",
    "    \n",
    "    def _process_dialogue_log(self, sample, dialogue_log):\n",
    "\n",
    "        dialog_history_memory = []\n",
    "        dialog_history = \"\"\n",
    "        domains = self._get_domains_from_log(dialogue_log)\n",
    "        slots = self._get_slots_from_domains(domains, \n",
    "                                             self.with_slot_description,\n",
    "                                             self.with_req_inf_differentiation,\n",
    "                                             self.with_all_slots) # or all\n",
    "\n",
    "        for turn_nb, turn in enumerate(dialogue_log):\n",
    "\n",
    "            if turn_nb % 2 == 0:\n",
    "                speaker = \"USER\"\n",
    "            else:\n",
    "                speaker = \"SYSTEM\"\n",
    "\n",
    "            utterance = f\"\"\"{speaker}: {turn[\"text\"]}\\n\"\"\"\n",
    "            dialogue_context = dialog_history + utterance\n",
    "            dialog_act = turn[\"dialog_act\"]\n",
    "            prompt = self._build_prompt(mode=\"dst\",\n",
    "                                        slots=slots,\n",
    "                                        dialogue_context=dialogue_context) \n",
    "\n",
    "\n",
    "            if self.dialog_history_limit != 0:\n",
    "                if self.dialog_history_limit == -1:\n",
    "                    self.dialog_history_limit = len(dialogue_log)\n",
    "\n",
    "                if len(dialog_history_memory) >= self.dialog_history_limit:\n",
    "                    dialog_history_memory.pop(0)\n",
    "                dialog_history_memory.append(utterance)\n",
    "                dialog_history = \"\".join(dialog_history_memory)\n",
    "\n",
    "            self.idx += 1\n",
    "            if turn_nb % 2 == 0:\n",
    "                self.dataset[\"gold_bs\"].append(dialog_act)\n",
    "                self.dataset[\"dialogue_context\"].append(dialogue_context)\n",
    "                self.dataset[\"gold_database_result\"].append(None) \n",
    "                self.dataset[\"turn\"].append(turn_nb//2)\n",
    "                self.dataset[\"domains\"].append(domains)\n",
    "                self.dataset[\"id\"].append(self.idx//2)\n",
    "                self.dataset[\"dialogue_id\"].append(sample)\n",
    "                self.dataset[\"prompt\"].append(prompt)\n",
    "            else:\n",
    "                self.dataset[\"gold_response\"].append(utterance)\n",
    "                self.dataset[\"gold_act\"].append(dialog_act)\n",
    "\n",
    "\n",
    "    def _get_domains_from_log(self, dialogue_log):\n",
    "        domains = []\n",
    "        all_domains = [\"restaurant\", \"taxi\", \"hotel\", \"train\", \"attraction\"]\n",
    "        for log in dialogue_log:\n",
    "            for domain_act in log[\"dialog_act\"]:\n",
    "                domain = domain_act.split(\"-\")[0].lower()\n",
    "                if domain in all_domains and domain not in domains:\n",
    "                    domains.append(domain)\n",
    "        return domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mwoz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10438/10438 [00:00<00:00, 13068.15it/s]\n"
     ]
    }
   ],
   "source": [
    "mwoz_path = \"/home/willy/instructod/MultiWOZ_2.1/\"\n",
    "dialog_history_limit = 0\n",
    "single_domain_only = False\n",
    "with_slot_description = False\n",
    "with_req_inf_differentiation = False\n",
    "with_all_slots = True\n",
    "mwoz = MWOZ_Dataset(CONFIG, \n",
    "                    mwoz_path,\n",
    "                    dialog_history_limit,\n",
    "                    with_slot_description,\n",
    "                    with_req_inf_differentiation,\n",
    "                    single_domain_only,\n",
    "                    with_all_slots)\n",
    "dataset = mwoz.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to utilize.\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The path of the HuggingFace model.\"}\n",
    "    )\n",
    "    use_int8: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use int8 model or not.\"}\n",
    "    )\n",
    "    use_deepspeed: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use deepspeed model or not.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(model_name_or_path=\"google/flan-t5-xxl\",\n",
    "                            use_int8=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline(model_args):\n",
    "    print(f'Loading {model_args.model_name_or_path}...')\n",
    "    \n",
    "    if model_args.use_int8:\n",
    "        if \"t5\" in model_args.model_name_or_path or \"flan-ul\" in model_args.model_name_or_path:\n",
    "            tokenizer = T5Tokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\n",
    "                model_args.model_name_or_path, device_map=\"auto\", load_in_8bit=True)\n",
    "            generator = pipeline(\n",
    "                task=\"text2text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "            # results = generator(prompt_dict[\"prompts\"])\n",
    "            # prompt_dict[\"results\"] = [result[\"generated_text\"] for result in results]\n",
    "            return generator\n",
    "        elif \"flan-alpaca\" in model_args.model_name_or_path:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_args.model_name_or_path, device_map=\"auto\", load_in_8bit=True)\n",
    "            generator = pipeline(\n",
    "                task=\"text2text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "            # results = generator(prompt_dict[\"prompts\"], max_length=300)\n",
    "            # prompt_dict[\"results\"] = [result[\"generated_text\"] for result in results]\n",
    "            return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/flan-t5-xxl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ff4e42d79947658bb7046b0536961e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"spiece.model\";:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c47918bb26d42e78f5c14a55ff17cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626f8160dbb343f2a8d12646f6c91400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888b37398886498897917e00f4478663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c222e36131b4fd3ac8606618a70540d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/50.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa20bfac20bc4342b811f7cf09763f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)00001-of-00005.bin\";:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d8b33000ce446392d35475fb5b5ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)00002-of-00005.bin\";:   0%|          | 0.00/9.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = load_pipeline(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = list(dataset[\"prompt\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"slot1:'destination', 'Pizza Hut Fen Dit\"},\n",
       " {'generated_text': \"slot1:'leaveat', '17:15'\"},\n",
       " {'generated_text': 'SYSTEM: Thank you!'},\n",
       " {'generated_text': 'SYSTEM: You are all set.'},\n",
       " {'generated_text': 'SYSTEM: Nusha is a restaurant.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
