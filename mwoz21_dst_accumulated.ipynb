{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from src.DST.evaluate_utils import remapping\n",
    "from src.DST.dst import SLOTS_DESCRIPTIONS\n",
    "from src.config import CONFIG\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from transformers import TrainingArguments\n",
    "from src.DST.evaluate_utils import unpack_belief_states\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to utilize.\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The path of the HuggingFace model.\"}\n",
    "    )\n",
    "    use_int8: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use int8 model or not.\"}\n",
    "    )\n",
    "    use_deepspeed: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use deepspeed model or not.\"}\n",
    "    )\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to the data loading and preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Train dataset path\"}\n",
    "    )\n",
    "    dataset_names: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Train dataset paths\"}\n",
    "    )\n",
    "    root_data_path: Optional[str] = field(\n",
    "        default=\"./data\", metadata={\"help\": \"The path to the data directory.\"},\n",
    "    )\n",
    "    mwoz_path: Optional[str] = field(\n",
    "        default=\"/home/willy/instructod/MultiWOZ_2.1/\",\n",
    "        metadata={\"help\": \"MWOZ path\"}\n",
    "    )\n",
    "    dialog_history_limit_dst: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Lenght of dialogue history for dst\"}\n",
    "    )\n",
    "    dialog_history_limit_dst_recorrect: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Lenght of dialogue history for dst update\"}\n",
    "    )\n",
    "    dialog_history_limit_rg: Optional[int] = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"Lenght of dialogue history for response generation\"}\n",
    "    )\n",
    "    dialog_history_limit_e2e: Optional[int] = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"Lenght of dialogue history for e2e\"}\n",
    "    )\n",
    "    single_domain_only: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to keep only the single domain sample or not\"}\n",
    "    )\n",
    "    with_slot_description: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use slot description or not for DST\"}\n",
    "    )\n",
    "    with_req_inf_differentiation: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to differentiate between require and inform slot for DST\"}\n",
    "    )\n",
    "    with_all_slots: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use all slots or not\"}\n",
    "    )\n",
    "    debug_mode: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"debug mode to only try 20 samples\"}\n",
    "    )\n",
    "    start_idx: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Starting index to restart the prediction if needed\"}\n",
    "    )\n",
    "    save_path: Optional[str] = field(\n",
    "        default=\"results/\",\n",
    "        metadata={\"help\": \"save path\"}\n",
    "    )\n",
    "    save_every: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\"help\": \"every step to save in case api fail\"}\n",
    "    )\n",
    "    db_format_type: Optional[str] = field(\n",
    "        default=\"1\",\n",
    "        metadata={\"help\": \"1 is more precise, 2 is more concise for db integration\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class PromptingArguments(TrainingArguments):\n",
    "    \"\"\"\n",
    "    Arguments pertraining to the prompting pipeline.\n",
    "    \"\"\"\n",
    "    output_dir: Optional[str] = field(\n",
    "        default=\"./out\",\n",
    "        metadata={\"help\": \"Output directory\"},\n",
    "    )\n",
    "    task: Optional[str] = field(\n",
    "        default=\"dst\",\n",
    "        metadata={\"help\": \"Task to perform\"}\n",
    "    )\n",
    "    max_requests_per_minute: Optional[int] = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"Max number of requests for OpenAI API.\"}\n",
    "    )\n",
    "    openai_api_key_name: Optional[str] = field(\n",
    "        default=\"OPENAI_API_KEY\",\n",
    "        metadata={\"help\": \"OpenAI API key name.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "data_args = DataArguments()\n",
    "data_args.dialog_history_limit_e2e = -1\n",
    "data_args.dialog_history_limit_rg = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptConstructor():\n",
    "    def __init__(self, \n",
    "                 config):\n",
    "        self.config = config\n",
    "        self.instructions = config[\"INSTRUCTIONS\"]\n",
    "        self.prompt_templates = config[\"PROMPT_TEMPLATES\"]\n",
    "        \n",
    "    def _get_slots_from_domains(self, domains, with_slot_description, with_req_inf_differentiation, with_all_slots):\n",
    "        # slot_description = self.config[\"slot_descrpition\"]\n",
    "        if with_all_slots:\n",
    "            domains = \"all\"\n",
    "\n",
    "        if with_slot_description:\n",
    "            with_req_inf_differentiation = False #Slot description is the discriminator\n",
    "\n",
    "        if domains == \"all\":\n",
    "            if with_req_inf_differentiation:\n",
    "                req_slots = \", \".join(self.config[\"multiwoz21\"][\"all_requestable_slots\"])\n",
    "                inf_slots = \", \".join(self.config[\"multiwoz21\"][\"all_informable_slots\"])\n",
    "            else:\n",
    "                slots = set(self.config[\"multiwoz21\"][\"all_requestable_slots\"] + \n",
    "                            self.config[\"multiwoz21\"][\"all_informable_slots\"])\n",
    "                slots = \", \".join(slots)\n",
    "        elif not isinstance(domains, list):\n",
    "            raise ValueError(\"\"\"Provided domain should be either 'all' or list of valid domain names:\n",
    "                                - for multiwoz2.1 and 2.4: taxi, restaurant, hotel, train, attraction \n",
    "                                - for SGD: To-do\"\"\")\n",
    "        else:\n",
    "            req_slots = \"\"\n",
    "            inf_slots = \"\"\n",
    "            domain_req_slots = []\n",
    "            domain_inf_slots = []\n",
    "            for domain in domains:\n",
    "                domain_req_slots += self.config[\"multiwoz21\"][\"requestable_slots\"][domain]\n",
    "                domain_inf_slots += self.config[\"multiwoz21\"][\"informable_slots\"][domain]\n",
    "            if with_req_inf_differentiation:\n",
    "                domain_req_slots = set(domain_req_slots)\n",
    "                domain_inf_slots = set(domain_inf_slots)\n",
    "                req_slots += \", \".join(domain_req_slots)\n",
    "                inf_slots += \", \".join(domain_inf_slots)\n",
    "            else:\n",
    "                slots = set(domain_req_slots + domain_inf_slots)\n",
    "                slots = \", \".join(slots)\n",
    "\n",
    "        if with_req_inf_differentiation:\n",
    "            slots_info = f\"Requestable slots: {req_slots}\\nInformable slots: {inf_slots}\"\n",
    "        else:\n",
    "            slots_info = f\"{slots}\"\n",
    "\n",
    "        if with_slot_description:\n",
    "            slots = slots.split(\", \")\n",
    "            slots_info = \"\"\n",
    "            for slot in slots:\n",
    "                if slot not in self.config[\"multiwoz21\"][\"all_informable_slots\"]:\n",
    "                    continue\n",
    "                slots_info += f\"name: {slot}, description: {SLOTS_DESCRIPTIONS[slot]}\\n\"\n",
    "            slots_info = slots_info[:-2]\n",
    "        \n",
    "        return slots_info\n",
    "    \n",
    "    \n",
    "    def _build_prompt(self, mode=\"\", dialogue_context=\"\", ontology=\"\", slots=\"\", dialogue_acts=\"\", belief_states=\"\", database=\"\"):\n",
    "        prompt = \"\"\n",
    "        if mode == \"dst\":\n",
    "            instruction = self.instructions[\"instruction_with_slots\"]\n",
    "            template_variables = self.prompt_templates[\"template_with_slots\"]\n",
    "            template = PromptTemplate(input_variables= template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])\n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                     slots=slots,\n",
    "                                     dialogue_context=dialogue_context)\n",
    "            \n",
    "        elif mode == \"dst_recorrect\":\n",
    "            instruction = self.instructions[\"instruction_with_slots_recorrect\"]\n",
    "            template_variables = self.prompt_templates[\"template_with_slots_recorrect\"]\n",
    "            template = PromptTemplate(input_variables= template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])            \n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                    slots=slots,\n",
    "                                    dialogue_context=dialogue_context,\n",
    "                                    belief_states=belief_states)\n",
    "            \n",
    "        elif mode == \"database_query\":\n",
    "            instruction = self.instructions[\"instruction_query_database\"]\n",
    "            template_variables = self.prompt_templates[\"template_query_database\"]\n",
    "            template = PromptTemplate(input_variables= template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])\n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                     belief_states=belief_states)\n",
    "            \n",
    "        elif mode == \"response_generation\":\n",
    "            example = self.config[\"EXAMPLES\"][\"response_generation\"]\n",
    "            \n",
    "            instruction = self.instructions[\"instruction_response_generation\"]\n",
    "            template_variables = self.prompt_templates[\"template_response_generation\"]\n",
    "            template = PromptTemplate(input_variables = template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])\n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                     example=example,\n",
    "                                     dialogue_context=dialogue_context)\n",
    "        elif mode == \"e2e\":\n",
    "            instruction = self.instructions[\"instruction_e2e\"]\n",
    "            template_variables = self.prompt_templates[\"template_e2e\"]\n",
    "            template = PromptTemplate(input_variables = template_variables[\"input_variables\"],\n",
    "                                      template = template_variables[\"template\"])\n",
    "            prompt = template.format(instruction=instruction,\n",
    "                                     database=database,\n",
    "                                     dialogue_context=dialogue_context)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"'mode' should be one of: [dst, dst_recorrect, database_query, response_generation, e2e]\")\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "\n",
    "class MWOZ_Dataset(PromptConstructor):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 data_args):\n",
    "        PromptConstructor.__init__(self, config)\n",
    "        self.dataset = {\"id\":[],\n",
    "                        \"dialogue_id\":[],\n",
    "                        \"dialogue_context\":[],\n",
    "                        \"turn\":[],\n",
    "                        \"prompt_dst\":[],\n",
    "                        \"prompt_dst_update\":[],\n",
    "                        \"prompt_rg\":[],\n",
    "                        \"prompt_e2e\":[],\n",
    "                        \"domains\":[],\n",
    "                        \"turn_domain\":[],\n",
    "                        \"gold_turn_bs\":[],\n",
    "                        \"gold_bs\":[],\n",
    "                        \"gold_act\":[],\n",
    "                        \"gold_response\":[],\n",
    "                        \"gold_database_result\":[],\n",
    "                        }\n",
    "        \n",
    "        print(\"Loading data...\")\n",
    "        self.all_data, self.testfiles, self.system_acts = self._get_mwoz_data(data_args.mwoz_path)\n",
    "        print(\"Loading databases...\")\n",
    "        self.dbs_lexicalized = self._get_dbs_lexicalized(data_args.mwoz_path)\n",
    "        self.idx = 0\n",
    "        self.dialog_history_limit_dst = data_args.dialog_history_limit_dst\n",
    "        self.dialog_history_limit_rg = data_args.dialog_history_limit_rg\n",
    "        self.dialog_history_limit_e2e = data_args.dialog_history_limit_e2e\n",
    "        self.single_domain_only = data_args.single_domain_only\n",
    "        self.with_slot_description = data_args.with_slot_description\n",
    "        self.with_req_inf_differentiation = data_args.with_req_inf_differentiation\n",
    "        self.with_all_slots = data_args.with_all_slots\n",
    "        self.all_domains = [\"restaurant\", \"taxi\", \"hotel\", \"train\", \"attraction\"]\n",
    "\n",
    "        print(\"Processing mwoz...\")\n",
    "        for sample in tqdm(self.all_data):\n",
    "            if sample in self.testfiles:\n",
    "                dialogue_log = self.all_data[sample][\"log\"]\n",
    "                self._process_dialogue_log(sample=sample,\n",
    "                                           dialogue_log=dialogue_log)\n",
    "\n",
    "        self.dataset = pd.DataFrame(self.dataset)\n",
    "        if self.single_domain_only:\n",
    "            for index, row in tqdm(self.dataset.iterrows()):\n",
    "                if len(row[\"domains\"]) != 1:\n",
    "                    self.dataset.drop(index, inplace=True)\n",
    "\n",
    "                    \n",
    "    def _get_mwoz_data(self, mwoz_path):\n",
    "        data_path = os.path.join(mwoz_path, \"data.json\")\n",
    "        testListFile_path = os.path.join(mwoz_path, \"testListFile.txt\")\n",
    "        system_acts_path = os.path.join(mwoz_path, \"system_acts.json\")\n",
    "\n",
    "        with open(data_path, \"r\") as f:\n",
    "            all_data = json.load(f)\n",
    "            \n",
    "        with open(testListFile_path, \"r\") as f:\n",
    "            testfiles = f.read()\n",
    "        testfiles = testfiles.split(\"\\n\")\n",
    "        \n",
    "        with open(system_acts_path, \"r\") as f:\n",
    "            system_acts = json.load(f)\n",
    "            \n",
    "        return all_data, testfiles, system_acts\n",
    "    \n",
    "    def _get_dbs_lexicalized(self, mwoz_path):\n",
    "        domains = [\"restaurant\", \"hotel\", \"train\", \"attraction\"]\n",
    "        keep_data = {\"restaurant\":[\"address\", \"area\", \"food\", \"name\", \"pricerange\", \"phone\", \"postcode\"],\n",
    "                    \"attraction\":[\"name\", \"area\", \"address\", \"type\", \"postcode\"],\n",
    "                    \"hotel\":[\"name\", \"address\", \"area\", \"phone\", \"postcode\", \"pricerange\", \"stars\"],\n",
    "                    \"train\":[\"departure\", \"destination\"]}\n",
    "        dbs_lexicalized = {}\n",
    "        for domain in domains:\n",
    "            db_path = os.path.join(mwoz_path, f\"{domain}_db.json\")\n",
    "            with open(db_path, \"r\") as f:\n",
    "                db_data = json.load(f)\n",
    "\n",
    "            db_lexicalized = []\n",
    "            for row in db_data:\n",
    "                row_keep = []\n",
    "                for key in keep_data[domain]:\n",
    "                        if key in row:\n",
    "                            row_keep.append(f\"{key}: {row[key]}\")\n",
    "                db_lexicalized.append(\", \".join(row_keep))\n",
    "            dbs_lexicalized[domain] = \"\\n\".join(set(db_lexicalized))\n",
    "\n",
    "        return dbs_lexicalized\n",
    "    \n",
    "    def _process_dialogue_log(self, sample, dialogue_log):\n",
    "\n",
    "        dialog_history_memory_dst = []\n",
    "        dialog_history_memory_rg = []\n",
    "        dialog_history_memory_e2e = []\n",
    "        dialog_history_dst = \"\"\n",
    "        dialog_history_rg = \"\"\n",
    "        dialog_history_e2e = \"\"\n",
    "        turn_domain = \"\"\n",
    "        domains = self._get_domains_from_log(dialogue_log)\n",
    "        slots = self._get_slots_from_domains(domains, \n",
    "                                             self.with_slot_description,\n",
    "                                             self.with_req_inf_differentiation,\n",
    "                                             self.with_all_slots) # or all\n",
    "\n",
    "        for turn_nb, turn in enumerate(dialogue_log):\n",
    "\n",
    "            if turn_nb % 2 == 0:\n",
    "                speaker = \"USER\"\n",
    "            else:\n",
    "                speaker = \"SYSTEM\"\n",
    "            \n",
    "            utterance = f\"\"\"{speaker}: {turn[\"text\"]}\\n\"\"\"\n",
    "            dialog_act = turn[\"dialog_act\"]\n",
    "            \n",
    "            dialogue_context_dst = dialog_history_dst + utterance\n",
    "            prompt_dst = self._build_prompt(mode=\"dst\",\n",
    "                                            slots=slots,\n",
    "                                            dialogue_context=dialogue_context_dst)\n",
    "            \n",
    "            lexicalized_act = self._lexicalize_act(dialog_act)\n",
    "            dialogue_context_rg = dialog_history_rg + utterance + f\"ACT:{lexicalized_act}\\nSYSTEM:\"\n",
    "            prompt_rg = self._build_prompt(mode=\"response_generation\",\n",
    "                                            dialogue_context=dialogue_context_rg)\n",
    "            \n",
    "            dialogue_context_e2e = dialog_history_e2e + utterance + \"SYSTEM:\"\n",
    "            # need to have utterance level domain here\n",
    "            cur_system_act = self.system_acts[sample.split(\".\")[0]][str((turn_nb//2)+1)]\n",
    "            turn_domain = self._get_domain_from_turn(turn_domain, cur_system_act)\n",
    "            if turn_domain and turn_domain != \"taxi\":\n",
    "                database = self.dbs_lexicalized[turn_domain]\n",
    "            else:\n",
    "                database = \"\"\n",
    "            prompt_e2e = self._build_prompt(mode=\"e2e\",\n",
    "                                            database=database,\n",
    "                                            dialogue_context=dialogue_context_e2e)\n",
    "\n",
    "            dialog_history_dst, dialog_history_memory_dst = self._update_dialogue_memory(utterance, \n",
    "                                                                                         dialogue_log, \n",
    "                                                                                         self.dialog_history_limit_dst, \n",
    "                                                                                         dialog_history_memory_dst)\n",
    "            dialog_history_rg, dialog_history_memory_rg = self._update_dialogue_memory(utterance, \n",
    "                                                                                       dialogue_log, \n",
    "                                                                                       self.dialog_history_limit_rg,\n",
    "                                                                                       dialog_history_memory_rg)\n",
    "            dialog_history_e2e, dialog_history_memory_e2e = self._update_dialogue_memory(utterance, \n",
    "                                                                                         dialogue_log, \n",
    "                                                                                         self.dialog_history_limit_e2e, \n",
    "                                                                                         dialog_history_memory_e2e) \n",
    "                \n",
    "            metadata = turn[\"metadata\"]\n",
    "            bspn_dict = {}\n",
    "            if metadata:\n",
    "                for domain in metadata:\n",
    "                    slot_values = metadata[domain][\"semi\"]\n",
    "                    for slot in slot_values:\n",
    "                        value = slot_values[slot]\n",
    "                        if value and value not in [\"not mentioned\", \"none\"]:\n",
    "                            if domain in bspn_dict:\n",
    "                                bspn_dict[domain].append(remapping(slot))\n",
    "                                bspn_dict[domain].append(remapping(value))\n",
    "                            else:\n",
    "                                bspn_dict[domain] = [remapping(slot), remapping(value)]\n",
    "                bspn = \" \".join([f\"[{domain}] {' '.join(bspn_dict[domain])}\" for domain in bspn_dict])\n",
    "\n",
    "            self.idx += 1\n",
    "            if turn_nb % 2 == 0:\n",
    "                self.dataset[\"gold_turn_bs\"].append(dialog_act)\n",
    "                self.dataset[\"dialogue_context\"].append(dialogue_context_dst)\n",
    "                self.dataset[\"gold_database_result\"].append(None) \n",
    "                self.dataset[\"turn\"].append(turn_nb//2)\n",
    "                self.dataset[\"domains\"].append(domains)\n",
    "                self.dataset[\"id\"].append(self.idx//2)\n",
    "                self.dataset[\"dialogue_id\"].append(sample)\n",
    "                self.dataset[\"prompt_dst\"].append(prompt_dst)\n",
    "                self.dataset[\"prompt_dst_update\"].append(prompt_dst)\n",
    "                self.dataset[\"prompt_rg\"].append(prompt_rg)\n",
    "                self.dataset[\"prompt_e2e\"].append(prompt_e2e)\n",
    "                self.dataset[\"turn_domain\"].append(turn_domain)\n",
    "            else:\n",
    "                self.dataset[\"gold_response\"].append(utterance)\n",
    "                self.dataset[\"gold_bs\"].append(bspn)\n",
    "                self.dataset[\"gold_act\"].append(dialog_act)\n",
    "\n",
    "    def _update_dialogue_memory(self, utterance, dialogue_log, dialog_history_limit, dialog_history_memory):\n",
    "        if dialog_history_limit != 0:\n",
    "            if dialog_history_limit == -1:\n",
    "                dialog_history_limit = len(dialogue_log)\n",
    "            if len(dialog_history_memory) >= dialog_history_limit:\n",
    "                dialog_history_memory.pop(0)\n",
    "            dialog_history_memory.append(utterance)\n",
    "\n",
    "        dialog_history = \"\".join(dialog_history_memory)\n",
    "        return dialog_history, dialog_history_memory\n",
    "    \n",
    "    def _lexicalize_act(self, act):\n",
    "        lexicalized_acts = []\n",
    "        lexicalize_mapping = {\"leave\": \"leave time\",\n",
    "                              \"arrive\":\"arrival time\",\n",
    "                              \"departure\":\"departure place\",\n",
    "                              \"post\":\"postcode\",\n",
    "                              \"addr\":\"address\"}\n",
    "\n",
    "        for act, slot_values in act.items():\n",
    "\n",
    "\n",
    "            if \"request\" in act.lower():\n",
    "                requests = []\n",
    "                for (slot, value) in slot_values:\n",
    "                    slot = slot.lower()\n",
    "                    if slot in lexicalize_mapping:\n",
    "                        slot = lexicalize_mapping[slot]\n",
    "                    if slot == \"none\":\n",
    "                        break\n",
    "                    else:\n",
    "                        requests.append(slot)\n",
    "                if requests:\n",
    "                    lexicalized_act = \"Request the user about \" + \", \".join(requests) + \".\"\n",
    "                    lexicalized_acts.append(lexicalized_act)\n",
    "\n",
    "            elif \"recommend\" in act.lower():\n",
    "                recommends = []\n",
    "                for (slot, value) in slot_values:\n",
    "                    slot, value = slot.lower(), value.lower()\n",
    "                    if slot in lexicalize_mapping:\n",
    "                        slot = lexicalize_mapping[slot]\n",
    "                    if slot == \"none\":\n",
    "                        break\n",
    "                    else:\n",
    "                        recommends.append(value)\n",
    "                if recommends:\n",
    "                    lexicalized_act = \"Recommend the user for \" + \", \".join(recommends) + \".\"\n",
    "                    lexicalized_acts.append(lexicalized_act)\n",
    "\n",
    "            elif \"inform\" in act.lower():\n",
    "                informs = []\n",
    "                for (slot, value) in slot_values:\n",
    "                    slot, value = slot.lower(), value.lower()\n",
    "                    if slot in lexicalize_mapping:\n",
    "                        slot = lexicalize_mapping[slot]\n",
    "                    if slot == \"none\":\n",
    "                        break\n",
    "                    else:\n",
    "                        informs.append(f\"the {slot} is {value}\")\n",
    "                if informs:\n",
    "                    lexicalized_act = \"Inform the user that \" + \", \".join(informs) + \".\"  \n",
    "                    lexicalized_acts.append(lexicalized_act)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "        if lexicalized_acts:\n",
    "            return \" \".join(lexicalized_acts)\n",
    "        else:\n",
    "            return \"None\"\n",
    "        \n",
    "    def _get_domain_from_turn(self, domain, act):\n",
    "        for k in act:\n",
    "            turn_domain = k.lower().split(\"-\")[0]\n",
    "            if turn_domain in self.all_domains:\n",
    "                return turn_domain\n",
    "        return domain\n",
    "            \n",
    "\n",
    "    def _get_domains_from_log(self, dialogue_log):\n",
    "        domains = []\n",
    "        for log in dialogue_log:\n",
    "            for domain_act in log[\"dialog_act\"]:\n",
    "                domain = domain_act.split(\"-\")[0].lower()\n",
    "                if domain in self.all_domains and domain not in domains:\n",
    "                    domains.append(domain)\n",
    "        return domains\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz = MWOZ_Dataset(CONFIG, data_args)\n",
    "dataset = mwoz.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(\"/home/willy/instructod/src/DST/results_single/gpt-3.5-turbo_0-end_singleDomainOnlyTrue_withSlotDescriptionFalse_withSlotDifferentiationFalse_dialogHistoryLimit0_prompt3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    id = df_results[\"id\"][i]\n",
    "    pred = df_results[\"preds\"][i]\n",
    "    unpacked_pred = unpack_belief_states(pred, \"pred\")\n",
    "    row = dataset.loc[dataset[\"id\"] == id]\n",
    "    print(\"unpacked pred: \", unpacked_pred)\n",
    "    print(\"turn domain: \", row[\"turn_domain\"].item())\n",
    "    print(\"gold belief state: \", row[\"gold_bs\"].item())\n",
    "    print(\"-------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
